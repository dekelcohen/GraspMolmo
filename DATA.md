# PRISM

The PRISM dataset is available for download on [ðŸ¤— HuggingFace](https://huggingface.co/datasets/allenai/PRISM). Refer to the dataset page for relevant details.

While the huggingface dataset contains rendered scenes and observations which are easier to use, we also supply the original 3D scenes for the sake of completeness.
If desired, this can be used to regenerate observations, potentially with a different or more realistic renderer.

This data can be downloaded here:
- [PRISM-Train](https://pub-3e61ad92c7024712b84e4bf8658147f7.r2.dev/PRISM_train.tar)
- [PRISM-Test](https://pub-3e61ad92c7024712b84e4bf8658147f7.r2.dev/PRISM_test.tar)
- [Splits](https://pub-3e61ad92c7024712b84e4bf8658147f7.r2.dev/PRISM_splits.tar) (train/test split of each object instance in each class)

If you want to generate entirely new scenes, the following data is required:
- [Grasp Descriptions](https://pub-3e61ad92c7024712b84e4bf8658147f7.r2.dev/annotations.tar.gz) (natural-language descriptions of each grasp)
- [Tasks JSON](https://pub-3e61ad92c7024712b84e4bf8658147f7.r2.dev/tasks.json) (generated tasks and grasps that would accomplish them)

## Data Overview

Each of the above splits contains:
- Procedurally generated 3D scenes (output of `datagen.py`), containing geometry, arrangement, grasp, view pose, and lighting information.
- Tasks matched with grasps in the viewpoints of the generated scenes

### Pregenerated scenes

The scenes data in each split is structured as follows:
```
<scene_id>
  scene.pkl (contains all scene information)
  metadata.yaml (contains list of graspable objects in scene)
```

Each `scene.pkl` contains a dictionary with the following structure:
```
{
    "annotations": {
        <annot_id>: [Annotation object, 4x4 grasp pose in world frame, 3D point in world frame being grasped]
    },
    "lighting": {
        [
            {
                "type": "DirectionalLight",
                "args": {
                    "name": "light",
                    "color": [r, g, b] # 0-255
                    "intensity": float
                },
                "transform": 4x4 pose matrix of light
            },
            ...
        ]
    },
    "views": [
        {
            "cam_K": 3x3 camera intrinsics matrix,
            "cam_pose": 4x4 camera pose matrix (note that the camera z-axis points backwards),
            "annotations_in_view": list[str] of <annot_ids> in annotations dict which are visible from this view
        },
        ...
    ]
    "glb": base64-encoded GLB string of 3D scene,
    "img_size": (height, width)
}
```

### Rendering observations

Notably, this data does not include the rendered views, which are available on HuggingFace. These views can also be rendered with:

```bash
python graspmolmo/datagen/generate_obs.py scene_dir=<path_to_data> out_dir=<output_path>
```

This will generate downstream information from the scene data. Each scene will result in a single `<scene_id>.hdf5` file, with the following structure:

```
view_<i>/
    rgb: RGB image as (H,W,3) array
    xyz: Back-projected point-cloud (from RGB-D view) as (H,W,3) array of XYZ points
    seg: Segmentation map as (H,W) array where each pixel is index of object name in object_names
    object_names: List of object names visible in view
    normals (optional): Point-cloud normals as (H,W,3) array
    view_pose: Camera pose in world frame as (4,4) array
    cam_params: Camera intrinsics matrix as (3,3) array

    obs_<j>/
        grasp_pose: Grasp pose in camera frame as (4,4) array
        grasp_point: Point being grasped in camera frame as (3,) array
        grasp_point_px: Point being grasped projected onto image plane as (2,) array
        annot: YAML-formatted object with the following keys: ["annotation_id", "grasp_description", "object_description", "object_category", "object_id", "grasp_id"]
```

### Pregenerated Task-Grasps

The tasks and matching grasps are pregenerated and included in the data download. This is in the form of a CSV file with the following columns:
```
scene_path: The file name of the scene .hdf5 file generated by generate_obs.py
scene_id: The scene ID
view_id: The view ID, i.e. the key in the scene HDF5
obs_id: The observation ID, i.e. the key in the view group in the scene HDF5
task: The natural language task description
original_grasp_desc: The original generated matching grasp description, included for posterity
matching_grasp_desc: The matched grasp description, which is the same as <view_id>/<obs_id>/annot["grasp_description"] in the scene HDF5.
```

The rows of this CSV are the samples of the dataset.

## Generating new data

New data can be generated if desired. Doing so requires setting up the full datagen pipeline.

1. First, we need to prepare the 3D assets and stable grasps
    1. Download the [ACRONYM grasps](https://drive.google.com/file/d/1OjykLD9YmnFdfYpH2qO8yBo-I-22vKwu/view?usp=sharing) and extract to `<acronym_dir>`
    2. Download the [ShapeNetSem meshes](https://www.shapenet.org/) and place in `<shapenet_dir>`
    3. Run `python graspmolmo/preprocess_shapenet.py <acronym_dir>/grasps <shapenet_dir> <processed_dir> --blacklist asset_blacklist.txt --sampling-categories-file all_categories.txt`
2. Download the grasp descriptions from above and extract to `<annots_dir>`
3. Download the task JSON from above and place at `<tasks_json_path>`
4. Modify the variables at the top of `scripts/datagen_pipeline.sh` as required:
    1. `DATASET_PATH` is the root of the outputted files
    2. `ASSETS_PATH` is `<processed_dir>`
    3. `ANNOTS_PATH` is `<annots_dir>`
    4. `TASKS_JSON` is `<tasks_json_path>`
5. Generate new train and test data with `./scripts/datagen_pipeline.sh`

# TaskGrasp-Image

The TaskGrasp-Image dataset is available to download [here](https://pub-3e61ad92c7024712b84e4bf8658147f7.r2.dev/taskgrasp_image.tar.gz).
The structure is largely the same as the [TaskGrasp](https://github.com/adithyamurali/TaskGrasp) dataset. Where point cloud registration succeeded,
there are additional files `scans/<object_id>/<scan_id>_registered_grasps.npy`, `scans/<object_id>/<scan_id>_segmented_pc.npy`, and `scans/<object_id>/<scan_id>_pc_to_img_trf.npy`, which respectively contain the array of 25 annotated grasps in the camera frame, the segmented single-view point cloud of the object in the camera frame, and the transform between the camera frame and the fused point cloud frame.

For an example of how to visualize the TaskGrasp-Image data, please refer to [this example script](examples/visualize_tg_img.py).
